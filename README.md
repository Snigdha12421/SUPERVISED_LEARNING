# ML_CLASS_WORK

This repository contains **implementations of supervised learning regression algorithms** in Python.  
All algorithms are implemented using their **original mathematical formulas**, without using machine learning libraries such as `scikit-learn`.

---

## ğŸ“˜ Algorithms Implemented

---

## 1ï¸âƒ£ Simple Linear Regression (Ordinary Least Squares â€“ OLS)

**File:** `Simple_Linear_Regression(OLS).py`

### ğŸ“Œ Model
\[
\hat{y} = mx + b
\]

### ğŸ“ OLS Formulas
\[
m = \frac{\sum (x - \bar{x})(y - \bar{y})}{\sum (x - \bar{x})^2}
\]

\[
b = \bar{y} - m\bar{x}
\]

### ğŸ¯ Objective
Minimize the **sum of squared errors**:
\[
\sum (y - \hat{y})^2
\]

---

## 2ï¸âƒ£ Multiple Linear Regression (OLS â€“ Formula Method)

**File:** `Multiple_Linear_Regression(OLS).py`

### ğŸ“Œ Model
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]

### ğŸ“ OLS Coefficient Formulas

\[
\beta_1 =
\frac{
(\sum x_2^2)(\sum x_1 y) - (\sum x_1 x_2)(\sum x_2 y)
}{
(\sum x_1^2)(\sum x_2^2) - (\sum x_1 x_2)^2
}
\]

\[
\beta_2 =
\frac{
(\sum x_1^2)(\sum x_2 y) - (\sum x_1 x_2)(\sum x_1 y)
}{
(\sum x_1^2)(\sum x_2^2) - (\sum x_1 x_2)^2
}
\]

\[
\beta_0 = \bar{y} - \beta_1 \bar{x}_1 - \beta_2 \bar{x}_2
\]

---

## 3ï¸âƒ£ Multiple Linear Regression (Matrix / Normal Equation Method)

**File:** `Multiple_Linear_Regression(Matrix).py`

### ğŸ“Œ Model
\[
y = X\beta
\]

### ğŸ“ Normal Equation
\[
\beta = (X^T X)^{-1} X^T y
\]

### âš ï¸ Note
To avoid singular matrix errors, the **Mooreâ€“Penrose Pseudoinverse** is used:
\[
\beta = (X^T X)^{+} X^T y
\]

---

## 4ï¸âƒ£ Gradient Descent for Linear Regression

**File:** `Gradient_Descent.py`

### ğŸ“Œ Model
\[
\hat{y} = mx + b
\]

### ğŸ“‰ Cost Function (Mean Squared Error)
\[
J(m,b) = \frac{1}{n}\sum (y - \hat{y})^2
\]

### ğŸ” Update Rules
\[
m = m - \alpha \left(-\frac{2}{n}\sum x(y - \hat{y})\right)
\]

\[
b = b - \alpha \left(-\frac{2}{n}\sum (y - \hat{y})\right)
\]

Where:
- \(\alpha\) = learning rate  
- \(n\) = number of data points  

---
